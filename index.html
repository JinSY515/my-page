<!doctype html>
<html>
<head>
<!--
	 Global site tag (gtag.js) - Google Analytics 
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-141762792-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-141762792-1');
</script>
-->

<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Siyoon Jin's Homepage</title>
<link href="AboutPageAssets/styles/aboutPageStyle.css" rel="stylesheet" type="text/css">
<style type="text/css">
</style>

<!--The following script tag downloads a font from the Adobe Edge Web Fonts server for use within the web page. We recommend that you do not modify it.-->
<script>var __adobewebfontsappname__="dreamweaver"</script><script src="http://use.edgefonts.net/montserrat:n4:default;source-sans-pro:n2:default.js" type="text/javascript"></script>
</head>

<body alink = "#282727" vlink = "#9b9b9b" link = "#9b9b9b">
<!-- Header content -->
<header>
  <div class="profilePhoto"> 
    <!-- Profile photo --> 
    <img src="AboutPageAssets/images/photo.jpg" alt="sample" width="200" > 
  </div>
<!--	<img src="AboutPageAssets/images/fonitAigolkatan.jpg" alt="sample" width="259"> </div>-->
  <!-- Identity details -->
  <section class="profileHeader">
    <h1>Siyoon Jin</h1>
<!--    <h3>Undergraduate student</h3>-->
    <hr>
    <p>I am a 1st year MS student at Computer Vision Lab (CVLAB) under the supervision of Prof. <a href="https://cvlab.kaist.ac.kr/" target="_blank">
		Seungryong Kim</a> at <a href="https://www.kaist.ac.kr/kr/">Korea Advanced Institute of Science and Technology (KAIST)</a>. My research is in computer vision and deep learning, specifically interested in image/video generation, editing, personalization and matching.<br>
    </p>
  </section>
  <!-- Links to Social network accounts -->
  <aside class="socialNetworkNavBar">
	   <div class="socialNetworkNav"> 
      <!-- Add a Anchor tag with nested img tag here --> 
		   <a href="mailto:siyun515@kaist.ac.kr">
      <img src="AboutPageAssets/images/mail.png"  alt="sample" width="30" ></a> </div>
      <div class="socialNetworkNav">
      <!-- Add a Anchor tag with nested img tag here --> 
		<a href="https://github.com/JinSY515" target="_blank">
      <img src="AboutPageAssets/images/github.png" alt="sample" width="30"></a>  </div>
    <!-- <div class="socialNetworkNav"> 
		<a href="" target="_blank"> -->
      <!-- Add a Anchor tag with nested img tag here --> 
      <!-- <img src="AboutPageAssets/images/scholar.jpg"  alt="sample" width="30"></a>  </div>
	  	  <div class="socialNetworkNav"> -->
      <!-- Add a Anchor tag with nested img tag here --> 
	
	  <div class="socialNetworkNav">
		 <a href="https://www.linkedin.com/in/siyoon-jin-055a00345" target="_blank">
			 <img src="AboutPageAssets/images/linkedin.png"  alt="sample" width="30"> </a></div>

	  
  </aside>
</header>
<!-- content -->
<section class="mainContent"> 
  <!-- Contact details -->
  
  <!-- Previous experience details -->
  
  <section class="section2">
  <h2 class="sectionTitle">Publications</h2>
    <hr class="sectionTitleRule">
    <hr class="sectionTitleRule2">
    <div class="sectionContent">
        <video autoplay muted loop playsinline width="100%;" >
            <source src="AboutPageAssets/teasers/teaser_010.mp4" type="video/mp4">
        </video> 
        <video autoplay muted loop playsinline width="100%;" >
            <source src="AboutPageAssets/teasers/teaser_001.mp4" type="video/mp4">
        </video> 
	  </div>
    <section class="section2Content">
        <h2 class="sectionContentTitle">MATRIX: Mask Track Alignment for Interaction-aware Video Generation</h2>
        <h3 class="sectionContentSubTitle"><b><a>Siyoon Jin</a></b>, <a>Seongchan Kim</a>, <a>Dahyun Chung</a>, <a>Jaeho Lee</a>, <a>Hyunwook Choi</a>, <a>Jisu Nam</a>, <a>Jiyoung Kim</a>, <a>Seungryong Kim</a></h3>
		  	<h3 class="sectionContentSubTitle"><b style="color:darksalmon"><em>ICLR 2026</b>
        </em></h3>
    </section>

    <aside class="externalResourcesNav">
      <div class="dropdown"> <a href="https://cvlab-kaist.github.io/MATRIX/" target="_blank">Project Page</a></div>
      <div class="dropdown"> <a href="https://arxiv.org/pdf/2510.07310" target="_blank">ArXiv</a></div>
      <div class="dropdown"> <span>Abstract</span>
      <div class="dropdown-content">
        <p style="text-align:left;">
            Video DiTs have advanced video generation, yet they still struggle to model
multi-instance or subject-object interactions. This raises a key question: How
do these models internally represent interactions? To answer this, we curate
MATRIX-11K, a video dataset with interaction-aware captions and multi-instance
mask tracks. Using this dataset, we conduct a systematic analysis that formalizes
two perspectives of video DiTs: semantic grounding, via video-to-text attention,
which evaluates whether noun and verb tokens capture instances and their re-
lations; and semantic propagation, via video-to-video attention, which assesses
whether instance bindings persist across frames. We find both effects concentrate
in a small subset of interaction-dominant layers. Motivated by this, we introduce
MATRIX, a simple and effective regularization that aligns attention in specific
layers of video DiTs with multi-instance mask tracks from the MATRIX-11K
dataset, enhancing both grounding and propagation. We further propose Inter-
GenEval, an evaluation protocol for interaction-aware video generation. In ex-
periments, MATRIX improves both interaction fidelity and semantic alignment
while reducing drift and hallucination. Extensive ablations validate our design
choices. Codes and weights will be released.
	        </p></div>
        </div>

      </aside>
    <br>
 
    <div class="sectionContent">
			  <img src="AboutPageAssets/teasers/difftrack_teaser.png" alt="DiffTrack Teaser Image" width="100%;">
	  	</div>
      <section class="section2Content">
      		<h2 class="sectionContentTitle">Emergent Temporal Correspondence from Video Diffusion Transformers</h2>
      		<h3 class="sectionContentSubTitle"><a>Jisu Nam</a>*, <a>Soowon Son</a>*, <a>Dahyun Chung</a>, <a>Jiyoung Kim</a>, <b><a>Siyoon Jin</a></b>, <a>Junwha Hur</a>, <a>Seungryong Kim</a>
	 		</h3>
		  	<h3 class="sectionContentSubTitle"><b style="color:darksalmon"><em>NeurIPS 2025</b>
	 		</em></h3>
    	</section>

      <aside class="externalResourcesNav">
        <div class="dropdown"> <a href="https://cvlab-kaist.github.io/DiffTrack/" target="_blank">Project Page</a></div>
        <div class="dropdown"> <a href="https://arxiv.org/pdf/2506.17220" target="_blank">ArXiv</a></div>
        <div class="dropdown"> <span>Abstract</span>
        <div class="dropdown-content">
          <p style="text-align:left;">
            Recent advancements in video diffusion models based on Diffusion Transformers
(DiTs) have achieved remarkable success in generating temporally coherent videos.
Yet, a fundamental question persists: how do these models internally establish and
represent temporal correspondences across frames? We introduce DiffTrack, the
first quantitative analysis framework designed to answer this question. DiffTrack
constructs a dataset of prompt-generated video with pseudo ground-truth tracking
annotations and proposes novel evaluation metrics to systematically analyze how
each component within the full 3D attention mechanism of DiTs (e.g., representa-
tions, layers, and timesteps) contributes to establishing temporal correspondences.
Our analysis reveals that query-key similarities in specific, but not all, layers play
a critical role in temporal matching, and that this matching becomes increasingly
prominent during the denoising process. We demonstrate practical applications
of DiffTrack in zero-shot point tracking, where it achieves state-of-the-art perfor-
mance compared to existing vision foundation and self-supervised video models.
Further, we extend our findings to motion-enhanced video generation with a novel
guidance method that improves temporal consistency of generated videos without
additional training. We believe our work offers crucial insights into the inner
workings of video DiTs and establishes a foundation for further research and
applications leveraging their temporal understanding.
	        </p></div>
        </div>

      </aside>

      <br>
	  	<div class="sectionContent">
			  <img src="AboutPageAssets/teasers/vid-camedit_teaser.png" alt="Vid-CamEdit Teaser Image" width="100%;">
	  	</div>
      <section class="section2Content">
      		<h2 class="sectionContentTitle">Vid-CamEdit: Video Camera Trajectory Editing with Generative Rendering from Estimated Geometry</h2>
      		<h3 class="sectionContentSubTitle"><a>Junyoung Seo</a>, <a>Jisang Han</a>, <a>Jaewoo Jung</a>, <b><a>Siyoon Jin</a></b>, <a>Joungbin Lee</a>, <a>Takuya Narihira</a>, <a>Kazumi Fukuda</a>, <a>Takashi Shibuya</a>, <a>Donghoon Ahn</a>, <a>Shoukang Hu</a>, <a>Seungryong Kim</a>, <a>Yuki Mitsufuji</a>
	 		</h3>
		  	<h3 class="sectionContentSubTitle"><b style="color:darksalmon"><em>AAAI 2026</b>
	 		</em></h3>
    	</section>

      <aside class="externalResourcesNav">
        <div class="dropdown"> <a href="https://cvlab-kaist.github.io/Vid-CamEdit/" target="_blank">Project Page</a></div>
        <div class="dropdown"> <a href="https://arxiv.org/pdf/2506.13697" target="_blank">ArXiv</a></div>
        <div class="dropdown"> <span>Abstract</span>
        <div class="dropdown-content">
          <p style="text-align:left;">
              We introduce Vid-CamEdit, a novel framework for video
              camera trajectory editing, enabling the re-synthesis of
              monocular videos along user-defined camera paths. This
              task is challenging due to its ill-posed nature and the limited
              multi-view video data for training. Traditional reconstruc-
              tion methods struggle with extreme trajectory changes, and
              existing generative models for dynamic novel view synthesis
              cannot handle in-the-wild videos. Our approach consists of
              two steps: estimating temporally consistent geometry, and
              generative rendering guided by this geometry. By integrat-
              ing geometric priors, the generative model focuses on syn-
              thesizing realistic details where the estimated geometry is
              uncertain. We eliminate the need for extensive 4D training
              data through a factorized fine-tuning framework that sepa-
              rately trains spatial and temporal components using multi-
              view image and video data. Our method outperforms base-
              lines in producing plausible videos from novel camera tra-
              jectories, especially in extreme extrapolation scenarios on
              real-world footage.
	        </p></div>
        </div>

      </aside>

      <br>
	  <!-- SMM  -->
	  	<div class="sectionContent">
<!--	 		<img align="center" src="AboutPageAssets/images/teaser-multidiffusion.jpg" style="height: 100%; width: 100%; object-fit: contain"  alt="">-->
			<img src="AboutPageAssets/teasers/am-adapter_teaser.png" alt="AM-Adapter Teaser Image" width="100%;">

	  	</div>
      <section class="section2Content">
      		<h2 class="sectionContentTitle">Appearance Matching Adapter for Exemplar-based Semantic Image Synthesis</h2>
      		<h3 class="sectionContentSubTitle"><b><a>Siyoon Jin</a></b>, <a>Jisu Nam</a>, <a>Jiyoung Kim</a>, <a>Dahyun Chung</a>, <a>Yeongseok Kim</a>, <a>Joonhyung Park</a>, <a>Heonjeong Chu</a>, <a>Seungryong Kim</a> 
	 		</h3>
		  	<h3 class="sectionContentSubTitle"><b style="color:darksalmon"><em>ICCV 2025</b>
	 		</em></h3>
    	</section>

      <aside class="externalResourcesNav">


        <div class="dropdown"> <a href="https://cvlab-kaist.github.io/AM-Adapter/" target="_blank">Project Page</a>

        </div>
        
        <div class="dropdown"> <a href="https://arxiv.org/pdf/2412.03150" target="_blank">ArXiv</a>

        </div>
        <div class="dropdown"> <span>Abstract</span>
        <div class="dropdown-content">
          <p style="text-align:left;">Exemplar-based semantic image synthesis aims to generate images aligned with given semantic content while preserving the appearance of an exemplar image. Conventional structure-guidance models, such as ControlNet, are limited in that they cannot directly utilize exemplar images as input, relying instead solely on text prompts to control appearance. Recent tuning-free approaches address this limitation by transferring local appearance from the exemplar image to the synthesized image through implicit cross-image matching in the augmented self-attention mechanism of pre-trained diffusion models. However, these methods face challenges when applied to content-rich scenes with significant geometric deformations, such as driving scenes. In this paper, we propose the Appearance Matching Adapter (AM-Adapter), a learnable framework that enhances cross-image matching within augmented self-attention by incorporating semantic information from segmentation maps. To effectively disentangle generation and matching processes, we adopt a stage-wise training approach. Initially, we train the structure-guidance and generation networks, followed by training the AM-Adapter while keeping the other networks frozen. During inference, we introduce an automated exemplar retrieval method to efficiently select exemplar image-segmentation pairs. Despite utilizing a limited number of learnable parameters, our method achieves state-of-the-art performance, excelling in both semantic alignment preservation and local appearance fidelity. Extensive ablation studies further validate our design choices. Code and pre-trained weights will be publicly available.	  </p></div>
        </div>

      </aside>

      
      <!-- SMM  -->
	  	<div class="sectionContent">
<!--	 		<img align="center" src="AboutPageAssets/images/teaser-multidiffusion.jpg" style="height: 100%; width: 100%; object-fit: contain"  alt="">-->
			<img src="AboutPageAssets/teasers/moditalker_teaser.png" alt="DreamMatcher Teaser Image" width="100%">

	  	</div>
	  	<section class="section2Content">
      		<h2 class="sectionContentTitle">MoDiTalker: Motion-Disentangled Diffusion Model for High-Fidelity Talking Head Generation</h2>
      		<h3 class="sectionContentSubTitle"> Seyeon Kim*, <b><a>Siyoon Jin</a>*</b>, <a>Jihye Park*</a>, <a>Kihong Kim</a>, <a>Jiyoung Kim</a>, <a>Jisu Nam</a>, <a>Seungryong Kim</a>
	 		</h3>
		  	<h3 class="sectionContentSubTitle"><b style="color:darksalmon"><em>AAAI 2025 </b>
	 		</em></h3>
    	</section>
		<aside class="externalResourcesNav">
      <div class="dropdown"> <a href="https://cvlab-kaist.github.io/MoDiTalker"  target="_blank">Project Page</a>

      </div>

      <div class="dropdown"> <a href="https://arxiv.org/pdf/2403.19144" target="_blank">ArXiv</a>

      </div>

		  <div class="dropdown"> <span>Abstract</span>
			<div class="dropdown-content">
			  <p style="text-align:left;">Conventional GAN-based models for talking head generation often suffer from limited quality and unstable training. Recent approaches based on diffusion models aim to address these limitations and improve fidelity. However, they still face challenges, including extensive sampling time and difficulties in maintaining temporal consistency due to the high stochasticity of diffusion models. To overcome these challenges, we propose a novel motion-disentangled diffusion model for high-quality talking head generation, dubbed <b>MoDiTalker</b>. We introduce two modules: the audio-to-motion (AToM), designed to generate synchronized lip motions from audio, and the motion-to-video (MToV), designed to produce high-quality talking head video following the generated motion.
          AToM excels in capturing subtle lip movements by leveraging an audio attention mechanism. In addition, MToV enhances temporal consistency by leveraging an efficient tri-plane representation. Our experiments conducted on standard benchmarks demonstrate that our model achieves superior performance compared to existing models. We also provide comprehensive ablation studies and user study results. 	  </p></div>
		  </div>
		 </aside>

     <br>
      <div class="sectionContent">
			<img src="AboutPageAssets/teasers/dreammatcher_teaser.png" alt="DreamMatcher Teaser Image" width="100%">

	  	</div>
	  	<section class="section2Content">
      		<h2 class="sectionContentTitle">DreamMatcher: Appearance Matching Self-Attention for Semantically-Consistent Text-to-Image Personalization
      		<h3 class="sectionContentSubTitle">Jisu Nam, <a>Heesu Kim</a>, <a>DongJae Lee</a>, <b><a>Siyoon Jin</a></b>, <a>Seungryong Kim</a>, <a>Seunggyu Chang</a> 
	 		</h3>
		  	<h3 class="sectionContentSubTitle"><b style="color:darksalmon"><em>CVPR 2024</b>
	 		</em></h3>
    	</section>
      <aside class="externalResourcesNav">

        <div class="dropdown"> <a href="https://cvlab-kaist.github.io/DreamMatcher"  target="_blank">Project Page</a>

        </div>

        <div class="dropdown"> <a href="https://arxiv.org/pdf/2402.09812" target="_blank">ArXiv</a>

        </div>
        <div class="dropdown"> <span>Abstract</span>
        <div class="dropdown-content">
          <p style="text-align:left;">The objective of Text-to-Image (T2I) personalization is to customize diffusion models to a user-provided reference concept, generating diverse images of the reference concept aligned with target prompts. Prior methods for T2I personalization utilize the tuning of pre-trained T2I model weights. However, they require a cumbersome tuning process for each concept before generating personalized images. To ease the use of T2I personalization, we propose DreamMatcher, a tuning-free plug-in method that provides semantically aligned visual conditions of the reference concept to a diffusion model during the denoising process. We find that the key and value in the self-attention module play important roles in generating the structure and appearance of a personalized image, respectively. Hence, DreamMatcher replaces the target values with reference values aligned by semantic matching, leaving the structure path unchanged to preserve the versatile capability of pre-trained T2I models for generating diverse structures. We also employ a semantic-consistent masking strategy to isolate regions irrelevant to the personalized concept, such as the background, from core regions that should be replaced by the aligned appearances. DreamMatcher is compatible with existing T2I models, showing significant improvements in complex, non-rigid scenarios. Intensive experiments demonstrate the effectiveness and superiority of our approach compared to other baselines.	  </p></div>
        </div>
       </aside>
	  <br><br>


  <p class="footerDisclaimer">Template of <a href="https://lioryariv.github.io/">Lior Yariv</a><span></span></p>
  <p class="footerNote"></p>
</footer>
</body>
</html>
